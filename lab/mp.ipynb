{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3316626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cyrusyc/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/mprester.py:182: UserWarning: mpcontribs-client not installed. Install the package to query MPContribs data, or construct pourbaix diagrams: 'pip install mpcontribs-client'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from materiathena.mp.agent import MPLLM\n",
    "\n",
    "\n",
    "mpllm = MPLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7112bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cyrusyc/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/mprester.py:182: UserWarning: mpcontribs-client not installed. Install the package to query MPContribs data, or construct pourbaix diagrams: 'pip install mpcontribs-client'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7zjGBauaslK0AZm49cDJT6LVFCZsG\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694946343,\n",
      "  \"model\": \"gpt-3.5-turbo-16k-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"search_materials_thermo__get\",\n",
      "          \"arguments\": \"{\\n  \\\"formula\\\": \\\"Fe3O4\\\",\\n  \\\"is_stable\\\": true,\\n  \\\"bandgap_min\\\": 0\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 15423,\n",
      "    \"completion_tokens\": 41,\n",
      "    \"total_tokens\": 15464\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "MPRestError",
     "evalue": "REST query returned with error status code 400 on URL https://api.materialsproject.org/materials/thermo/?is_stable=True&bandgap_min=0&formula=Fe3O4&_limit=1000 with message:\nRequest contains query parameters which cannot be used: bandgap_min",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMPRestError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/pscratch/sd/c/cyrusyc/materiathena/lab/mp.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter-p1.nersc.gov/pscratch/sd/c/cyrusyc/materiathena/lab/mp.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDo we have stable Cu-substituted lead phosphate apatite at 0K?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter-p1.nersc.gov/pscratch/sd/c/cyrusyc/materiathena/lab/mp.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is the bandgap of the most stable magnetite?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter-p1.nersc.gov/pscratch/sd/c/cyrusyc/materiathena/lab/mp.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m mpllm\u001b[39m.\u001b[39;49mrun_material_conversation(user_input, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-16k-0613\u001b[39;49m\u001b[39m\"\u001b[39;49m, debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/pscratch/sd/c/cyrusyc/materiathena/materiathena/mp/agent.py:358\u001b[0m, in \u001b[0;36mMPLLM.run_material_conversation\u001b[0;34m(self, user_input, model, debug)\u001b[0m\n\u001b[1;32m    356\u001b[0m function_to_call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaterial_routes[function_name]\n\u001b[1;32m    357\u001b[0m function_args \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response_message[\u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39marguments\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 358\u001b[0m function_response \u001b[39m=\u001b[39m function_to_call(query_params\u001b[39m=\u001b[39;49mfunction_args)\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m debug:\n\u001b[1;32m    361\u001b[0m     \u001b[39mprint\u001b[39m(function_response)\n",
      "File \u001b[0;32m/pscratch/sd/c/cyrusyc/materiathena/materiathena/mp/agent.py:104\u001b[0m, in \u001b[0;36mMPLLM.search_materials_thermo\u001b[0;34m(self, query_params)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m _fields:\n\u001b[1;32m    102\u001b[0m     query_params[\u001b[39m\"\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m query_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m\"\u001b[39m, []) \u001b[39m+\u001b[39m _fields\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmpr\u001b[39m.\u001b[39;49mthermo\u001b[39m.\u001b[39;49m_search(\n\u001b[1;32m    105\u001b[0m     num_chunks\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, chunk_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, all_fields\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mquery_params\n\u001b[1;32m    106\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/core/client.py:940\u001b[0m, in \u001b[0;36mBaseRester._search\u001b[0;34m(self, num_chunks, chunk_size, all_fields, fields, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A generic search method to retrieve documents matching specific parameters.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \n\u001b[1;32m    920\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[39m    A list of documents.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# This method should be customized for each end point to give more user friendly,\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# documented kwargs.\u001b[39;00m\n\u001b[0;32m--> 940\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_all_documents(\n\u001b[1;32m    941\u001b[0m     kwargs,\n\u001b[1;32m    942\u001b[0m     all_fields\u001b[39m=\u001b[39;49mall_fields,\n\u001b[1;32m    943\u001b[0m     fields\u001b[39m=\u001b[39;49mfields,\n\u001b[1;32m    944\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[1;32m    945\u001b[0m     num_chunks\u001b[39m=\u001b[39;49mnum_chunks,\n\u001b[1;32m    946\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/core/client.py:987\u001b[0m, in \u001b[0;36mBaseRester._get_all_documents\u001b[0;34m(self, query_params, all_fields, fields, chunk_size, num_chunks)\u001b[0m\n\u001b[1;32m    973\u001b[0m list_entries \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\n\u001b[1;32m    974\u001b[0m     (\n\u001b[1;32m    975\u001b[0m         (key, \u001b[39mlen\u001b[39m(entry\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    982\u001b[0m     reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    983\u001b[0m )\n\u001b[1;32m    985\u001b[0m chosen_param \u001b[39m=\u001b[39m list_entries[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(list_entries) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query_resource(\n\u001b[1;32m    988\u001b[0m     query_params,\n\u001b[1;32m    989\u001b[0m     fields\u001b[39m=\u001b[39;49mfields,\n\u001b[1;32m    990\u001b[0m     parallel_param\u001b[39m=\u001b[39;49mchosen_param,\n\u001b[1;32m    991\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[1;32m    992\u001b[0m     num_chunks\u001b[39m=\u001b[39;49mnum_chunks,\n\u001b[1;32m    993\u001b[0m )\n\u001b[1;32m    995\u001b[0m \u001b[39mreturn\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/core/client.py:288\u001b[0m, in \u001b[0;36mBaseRester._query_resource\u001b[0;34m(self, criteria, fields, suburl, use_document_model, parallel_param, num_chunks, chunk_size, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m url\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    286\u001b[0m             url \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_submit_requests(\n\u001b[1;32m    289\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    290\u001b[0m         criteria\u001b[39m=\u001b[39;49mcriteria,\n\u001b[1;32m    291\u001b[0m         use_document_model\u001b[39m=\u001b[39;49muse_document_model,\n\u001b[1;32m    292\u001b[0m         parallel_param\u001b[39m=\u001b[39;49mparallel_param,\n\u001b[1;32m    293\u001b[0m         num_chunks\u001b[39m=\u001b[39;49mnum_chunks,\n\u001b[1;32m    294\u001b[0m         chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[1;32m    295\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m    300\u001b[0m \u001b[39mexcept\u001b[39;00m RequestException \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/core/client.py:429\u001b[0m, in \u001b[0;36mBaseRester._submit_requests\u001b[0;34m(self, url, criteria, use_document_model, parallel_param, num_chunks, chunk_size, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m remaining_docs_avail \u001b[39m=\u001b[39m {}\n\u001b[1;32m    425\u001b[0m initial_params_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    426\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m: url, \u001b[39m\"\u001b[39m\u001b[39mverify\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: copy(crit)} \u001b[39mfor\u001b[39;00m crit \u001b[39min\u001b[39;00m new_criteria\n\u001b[1;32m    427\u001b[0m ]\n\u001b[0;32m--> 429\u001b[0m initial_data_tuples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_thread(\n\u001b[1;32m    430\u001b[0m     use_document_model, initial_params_list\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    433\u001b[0m \u001b[39mfor\u001b[39;00m data, subtotal, crit_ind \u001b[39min\u001b[39;00m initial_data_tuples:\n\u001b[1;32m    434\u001b[0m     subtotals\u001b[39m.\u001b[39mappend(subtotal)\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/core/client.py:643\u001b[0m, in \u001b[0;36mBaseRester._multi_thread\u001b[0;34m(self, use_document_model, params_list, progress_bar, timeout)\u001b[0m\n\u001b[1;32m    640\u001b[0m finished, futures \u001b[39m=\u001b[39m wait(futures, return_when\u001b[39m=\u001b[39mFIRST_COMPLETED)\n\u001b[1;32m    642\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m finished:\n\u001b[0;32m--> 643\u001b[0m     data, subtotal \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    645\u001b[0m     \u001b[39mif\u001b[39;00m progress_bar \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    646\u001b[0m         progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/.conda/envs/dev/lib/python3.11/site-packages/mp_api/client/core/client.py:745\u001b[0m, in \u001b[0;36mBaseRester._submit_request_and_process\u001b[0;34m(self, url, verify, params, use_document_model, timeout)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyError\u001b[39;00m, \u001b[39mIndexError\u001b[39;00m):\n\u001b[1;32m    743\u001b[0m         message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(data)\n\u001b[0;32m--> 745\u001b[0m \u001b[39mraise\u001b[39;00m MPRestError(\n\u001b[1;32m    746\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mREST query returned with error status code \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon URL \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m with message:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    748\u001b[0m )\n",
      "\u001b[0;31mMPRestError\u001b[0m: REST query returned with error status code 400 on URL https://api.materialsproject.org/materials/thermo/?is_stable=True&bandgap_min=0&formula=Fe3O4&_limit=1000 with message:\nRequest contains query parameters which cannot be used: bandgap_min"
     ]
    }
   ],
   "source": [
    "from materiathena.mp.agent import MPLLM\n",
    "\n",
    "\n",
    "mpllm = MPLLM()\n",
    "\n",
    "\n",
    "# user_input = input(\"Enter materials-related question: \")\n",
    "user_input = \"What is the bandgap of stable BaTiO3?\"\n",
    "user_input = \"What is the elastic tensor of NaCl?\"\n",
    "user_input = \"What is the bandgap of stable MgSe?\"\n",
    "user_input = \"Is YbCl3 magnetic or non-magnetic?\"\n",
    "user_input = \"What is the energy above hull of Cu3P4Pb3O16 at 0K?\"\n",
    "user_input = \"Search for the necessary information for FCC Cu and summarize for me.\"\n",
    "user_input = \"Is Cu stable in FCC or BCC structure at 0K based on the energy distance above hull?\"\n",
    "user_input = \"Do we have stable Cu-substituted lead phosphate apatite at 0K?\"\n",
    "user_input = \"What is the bandgap of the most stable magnetite?\"\n",
    "\n",
    "mpllm.run_material_conversation(user_input, model=\"gpt-3.5-turbo-16k-0613\", debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198fa7f",
   "metadata": {},
   "source": [
    "# Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openapi_schema_pydantic import PathItem, Operation, Parameter\n",
    "\n",
    "# class MPAPISpec:\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             force_download: bool = False,\n",
    "#             ):\n",
    "\n",
    "#         if force_download:\n",
    "#             spec = OpenAPISpec.from_url(\"https://api.materialsproject.org/openapi.json\")\n",
    "#             spec.\n",
    "\n",
    "# TODO: construct MPAPISpec class from OpenAPISpec\n",
    "\n",
    "\n",
    "def get_endpoints(spec: OpenAPISpec):\n",
    "    endpoints = [\n",
    "        (route, method, operation)\n",
    "        for route, paths in spec.paths.items()\n",
    "        for method, operation in paths\n",
    "        if method in [\"get\", \"post\"]\n",
    "    ]\n",
    "    return endpoints\n",
    "\n",
    "\n",
    "def get_functions(spec: OpenAPISpec):\n",
    "    endpoints = get_endpoints(spec)\n",
    "    functions = []\n",
    "    for route, method, operation in endpoints:\n",
    "        if operation is None:\n",
    "            continue\n",
    "\n",
    "        name = (\n",
    "            operation.operationId\n",
    "            if len(operation.operationId) <= 64\n",
    "            else operation.operationId[:64]\n",
    "        )\n",
    "\n",
    "        description = operation.description or operation.summary\n",
    "\n",
    "        properties = {\n",
    "            property.name: {\n",
    "                \"type\": property.param_schema.type,\n",
    "                \"description\": property.description\n",
    "                if property.description is not None\n",
    "                else \"\",\n",
    "                **(\n",
    "                    {\"enum\": property.param_schema.enum}\n",
    "                    if property.param_schema.enum is not None\n",
    "                    else {}\n",
    "                ),\n",
    "            }\n",
    "            for property in operation.parameters\n",
    "            if property.param_schema.type is not None\n",
    "        }\n",
    "\n",
    "        required = [\n",
    "            property.name for property in operation.parameters if property.required\n",
    "        ]\n",
    "        functions.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": properties,\n",
    "                },\n",
    "                **({\"required\": required} if required else {}),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return functions\n",
    "\n",
    "\n",
    "functions = get_functions(spec)\n",
    "\n",
    "print(len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"passed_functions.json\", \"w\") as f:\n",
    "#     json.dump(functions, f, indent=4)\n",
    "\n",
    "# functions\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"select_functions.json\", \"r\") as f:\n",
    "    functions = json.load(f)\n",
    "\n",
    "len(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e84068-df15-4050-9e9d-ccea43f82079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# from mp_api.client import MPRester\n",
    "\n",
    "# from langchain.tools import APIOperation, OpenAPISpec\n",
    "\n",
    "# load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "# MP_API_KEY = os.getenv(\"MP_API_KEY\", None)\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# spec = OpenAPISpec.from_url(\"https://api.materialsproject.org/openapi.json\")\n",
    "# operation = APIOperation.from_openapi_spec(\n",
    "#     spec, \"/materials/summary/{material_id}/\", \"get\"\n",
    "# )\n",
    "\n",
    "# # from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "# # from langchain.schema import HumanMessage, AIMessage, ChatMessage\n",
    "# # from langchain.tools import format_tool_to_openai_function\n",
    "\n",
    "# # llm = ChatAnthropic(anthropic_api_key=)\n",
    "# # llm = ChatOpenAI(model='gpt-4-0613', temperature=0.5, openai_api_key=OPENAI_API_KEY, client=)\n",
    "\n",
    "\n",
    "# class LLMaterialsAgent:\n",
    "#     def __init__(self, mp_api_key=MP_API_KEY, openai_api_key=OPENAI_API_KEY):\n",
    "#         # Initialize the Materials Project API\n",
    "#         self.mpr = MPRester(mp_api_key)\n",
    "#         # Initialize the OpenAI API\n",
    "#         openai.api_key = openai_api_key\n",
    "\n",
    "#     def get_materials_data(self, query_params):\n",
    "#         # Retrieve data from the Materials Project using the MPRester class\n",
    "#         data = self.mpr.summary.search(**query_params)\n",
    "#         return data\n",
    "\n",
    "#     def run_conversation(self, user_input):\n",
    "#         # Step 1: send the conversation and available functions to GPT\n",
    "#         messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "#         functions = [\n",
    "#             {\n",
    "#                 \"name\": \"get_materials_data\",\n",
    "#                 \"description\": \"Get materials data from the Materials Project\",\n",
    "#                 \"parameters\": {\n",
    "#                     \"type\": \"object\",\n",
    "#                     \"properties\": {\n",
    "#                         \"elements\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\"type\": \"string\"},\n",
    "#                             \"description\": \"Elements to query, e.g., ['Si', 'O']\",\n",
    "#                         },\n",
    "#                         \"band_gap\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\"type\": \"number\"},\n",
    "#                             \"description\": \"Range of band gap values, e.g., [0.5, 1.0]\",\n",
    "#                         },\n",
    "#                         \"fields\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\"type\": \"string\"},\n",
    "#                             \"description\": f\"Fields to return, including {self.mpr.summary.available_fields}\",\n",
    "#                         }\n",
    "#                         # \"limit\": {\n",
    "#                         #     \"type\": \"integer\",\n",
    "#                         #     \"description\": \"Number of materials to retrieve, e.g., 5\",\n",
    "#                         # },\n",
    "#                     },\n",
    "#                     \"required\": [\"elements\", \"band_gap\"],\n",
    "#                 },\n",
    "#             }\n",
    "#         ]\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-3.5-turbo-0613\",\n",
    "#             messages=messages,\n",
    "#             functions=functions,\n",
    "#             function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "#         )\n",
    "#         response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "#         # Step 2: check if GPT wanted to call a function\n",
    "#         if response_message.get(\"function_call\"):\n",
    "#             # Step 3: call the function\n",
    "#             # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "#             available_functions = {\n",
    "#                 \"get_materials_data\": self.get_materials_data,\n",
    "#             }  # only one function in this example, but you can have multiple\n",
    "#             function_name = response_message[\"function_call\"][\"name\"]\n",
    "#             function_to_call = available_functions[function_name]\n",
    "#             function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "#             function_response = function_to_call(query_params=function_args)\n",
    "\n",
    "#             # breakpoint()\n",
    "\n",
    "#             # Step 4: send the info on the function call and function response to GPT\n",
    "#             messages.append(\n",
    "#                 response_message\n",
    "#             )  # extend conversation with assistant's reply\n",
    "#             # breakpoint()\n",
    "#             messages.append(\n",
    "#                 {\n",
    "#                     \"role\": \"function\",\n",
    "#                     \"name\": function_name,\n",
    "#                     \"content\": json.dumps(function_response[0]),\n",
    "#                     # \"content\": function_response,\n",
    "#                 }\n",
    "#             )  # extend conversation with function response\n",
    "#             # breakpoint()\n",
    "#             second_response = openai.ChatCompletion.create(\n",
    "#                 model=\"gpt-3.5-turbo-0613\",\n",
    "#                 messages=messages,\n",
    "#             )  # get a new response from GPT where it can see the function response\n",
    "#             # breakpoint()\n",
    "#             return second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427454eb-7708-41ff-8b47-0607d836bd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# user_input = input(\"Please enter your natural language query: \")\n",
    "\n",
    "# user_input = \"Is UCl3 a stable material according to the thermodynamic hull on Materials Project?\"\n",
    "\n",
    "user_input = \"What is the bandgap of diamond based on Materials Project's data?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a data-vigilent agent that responds user requrests based on data hosted on Materials Project.\"\n",
    "            \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. Only use the functions you have been provided with.\"\n",
    "            \"Remember to provide `fields` or `_fields` to retrive as small request as possible.\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "]\n",
    "\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    # model=\"gpt-3.5-turbo-0613\",\n",
    "    model=\"gpt-3.5-turbo-16k-0613\",\n",
    "    # model=\"gpt-4-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    ")\n",
    "response_message = response[\"choices\"][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae61be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# passed_functions = []\n",
    "# for n in tqdm(range(len(functions))):\n",
    "#     try:\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-3.5-turbo-0613\",\n",
    "#             messages=messages,\n",
    "#             functions=[functions[n]],\n",
    "#             function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "#         )\n",
    "#         response_message = response[\"choices\"][0][\"message\"]\n",
    "#     except Exception as e:\n",
    "#         print(functions[n][\"name\"])\n",
    "#         continue\n",
    "#     passed_functions.append(functions[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf11272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(lambda x: x['name'], functions)).index('search_materials_synthesis__get')\n",
    "\n",
    "# functions.pop(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86962426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"passed_functions.json\", \"w\") as f:\n",
    "#     json.dump(functions, f, indent=4)\n",
    "\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x[\"name\"], functions)).index(\"search_materials_summary__get\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b2407",
   "metadata": {},
   "source": [
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_api.client.core import BaseRester\n",
    "from mp_api.client.routes import *\n",
    "\n",
    "\n",
    "class MPLLM:\n",
    "    def __init__(self, mp_api_key=MP_API_KEY, openai_api_key=OPENAI_API_KEY) -> None:\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "        self.base = BaseRester(\n",
    "            api_key=mp_api_key, monty_decode=True, use_document_model=False\n",
    "        )\n",
    "\n",
    "        self.materials = MaterialsRester(\n",
    "            api_key=mp_api_key, monty_decode=False, use_document_model=False\n",
    "        )\n",
    "\n",
    "        self.thermo = ThermoRester(\n",
    "            api_key=mp_api_key, monty_decode=False, use_document_model=False\n",
    "        )\n",
    "\n",
    "        self.tasks = TaskRester(\n",
    "            api_key=mp_api_key,\n",
    "        )\n",
    "\n",
    "        self.summary = SummaryRester(\n",
    "            api_key=mp_api_key, monty_decode=False, use_document_model=False\n",
    "        )\n",
    "\n",
    "    def search_base(self, query_params):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.base._search(all_fields=False, **query_params)\n",
    "\n",
    "    def search_materials_core(self, query_params: dict):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.materials._search(all_fields=False, **query_params)\n",
    "\n",
    "    def search_materials_summary(self, query_params: dict):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.summary._search(all_fields=False, **query_params)\n",
    "\n",
    "    def search_materials_thermo(self, query_params):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.thermo._search(all_fields=False, **query_params)\n",
    "\n",
    "\n",
    "mpllm = MPLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4c6ba-15f2-434c-8c3a-59c6b1cb5660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "    \"search_base\": mpllm.search_base,\n",
    "    \"search_materials_thermo__get\": mpllm.search_materials_thermo,\n",
    "    \"search_materials_core__get\": mpllm.search_materials_core,\n",
    "    \"search_materials_summary_stats__get\": mpllm.search_materials_core,\n",
    "    \"search_materials_summary__get\": mpllm.search_materials_summary,\n",
    "}  # only one function in this example, but you can have multiple\n",
    "function_name = response_message[\"function_call\"][\"name\"]\n",
    "function_to_call = available_functions[function_name]\n",
    "function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "function_response = function_to_call(query_params=function_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(function_to_call)\n",
    "print(function_args)\n",
    "\n",
    "response_message[\"function_call\"][\"arguments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monty.json import MontyEncoder, MontyDecoder\n",
    "\n",
    "# json.dumps(function_response[0], cls=MontyEncoder)\n",
    "# json.dumps(function_response[0].dict(), indent=4, cls=MontyDecoder)\n",
    "# function_response[0]\n",
    "# MontyEncoder().encode(function_response[0])\n",
    "# MontyDecoder().decode(function_response[0].__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e88f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7264c-6a2e-4461-b069-831d85480c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = json.dumps(function_response)\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"function\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": content,\n",
    "        # \"content\": function_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16860c9b-d7bb-4009-a048-973744d4765d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    ")  # get a new response from GPT where it can see the function response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffaad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214ab19-0304-489d-b5ed-b948cb87d444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymatgen.core import Structure, Element\n",
    "import datetime\n",
    "\n",
    "\n",
    "def serialize_nested_object(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        serialized_dict = {}\n",
    "        for key, value in obj.items():\n",
    "            serialized_dict[key] = serialize_nested_object(value)\n",
    "        return serialized_dict\n",
    "    elif isinstance(obj, list):\n",
    "        serialized_list = []\n",
    "        for item in obj:\n",
    "            serialized_list.append(serialize_nested_object(item))\n",
    "        return serialized_list\n",
    "    elif isinstance(obj, (datetime.datetime, Element)):\n",
    "        return str(obj)\n",
    "    elif isinstance(obj, (str, int, float)):\n",
    "        return obj\n",
    "    else:\n",
    "        # Handle other data types or custom objects as needed\n",
    "        return str(obj)\n",
    "\n",
    "\n",
    "serialize_nested_object(function_response[0].dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467a881-d1f7-443e-be86-bd123fd65740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monty.json import MontyEncoder\n",
    "\n",
    "encoder = MontyEncoder()\n",
    "a = encoder.encode(function_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf2dbc-d1e4-4f38-a690-78052d904c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monty.json import MontyDecoder, MontyEncoder\n",
    "\n",
    "decoder = MontyDecoder()\n",
    "decoder.decode(function_response[0].dict())\n",
    "# encoder = MontyEncoder()\n",
    "# encoder.encode()\n",
    "# json.dumps(function_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053d321-0599-4f20-9ffa-e0464ec4f3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
