{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7112bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPLLM is ready to serve you!\n",
      "User: What is the elastic tensor of NaCl?\n",
      "OpenAI: {\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"name\": \"search_materials_elasticity__get\",\n",
      "    \"arguments\": \"{\\n  \\\"formula\\\": \\\"NaCl\\\"\\n}\"\n",
      "  }\n",
      "}\n",
      "MP API call: search_materials_elasticity__get\n",
      "MP API args: {\n",
      "  \"formula\": \"NaCl\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf52265eb1a48a99aed90b894a321b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving MaterialsDoc documents:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188640b3b3543a485b8fed62836852e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving ElasticityDoc documents:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: {\n",
      "  \"pretty_formula\": \"NaCl\",\n",
      "  \"elasticity\": {\n",
      "    \"compliance_tensor\": [\n",
      "      [\n",
      "        1.682e-05,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        1.682e-05,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        1.682e-05,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ]\n",
      "    ],\n",
      "    \"elastic_tensor\": [\n",
      "      [\n",
      "        1.682e11,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        1.682e11,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        1.682e11,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        5.682e11,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        5.682e11,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        5.682e11\n",
      "      ]\n",
      "    ],\n",
      "    \"elastic_tensor_original\": [\n",
      "      [\n",
      "        1.682e11,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        1.682e11,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        1.682e11,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        5.682e11,\n",
      "        0.0,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        5.682e11,\n",
      "        0.0\n",
      "      ],\n",
      "      [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        5.682e11\n",
      "      ]\n",
      "    ],\n",
      "    \"elastic_tensor_voigt\": [\n",
      "      1.682e11,\n",
      "      1.682e11,\n",
      "      1.682e11,\n",
      "      5.682e11,\n",
      "      5.682e11,\n",
      "      5.682e11\n",
      "    ],\n",
      "    \"elastic_tensor_reuss\": [\n",
      "      1.682e11,\n",
      "      1.682e11,\n",
      "      1.682e11,\n",
      "      5.682e11,\n",
      "      5.682e11,\n",
      "      5.682e11\n",
      "    ],\n",
      "    \"elastic_tensor_vrh\": [\n",
      "      1.682e11,\n",
      "      1.682e11,\n",
      "      1.682e11,\n",
      "      5.682e11,\n",
      "      5.682e11,\n",
      "      5.682e11\n",
      "    ],\n",
      "    \"elastic_anisotropy\": 0.0,\n",
      "    \"poisson_ratio\": 0.5,\n",
      "    \"universal_anisotropy\": 0.0,\n",
      "    \"homogeneous_poisson\": true,\n",
      "    \"homogeneous_universal\": true,\n",
      "    \"warnings\": []\n",
      "  },\n",
      "  \"task_id\": \"mp-22862\"\n",
      "}\n",
      "User: Thank you\n"
     ]
    }
   ],
   "source": [
    "from llamp.mp.agent import MPLLM\n",
    "\n",
    "mpllm = MPLLM()\n",
    "\n",
    "print(\"MPLLM is ready to serve you!\")\n",
    "\n",
    "# user_input = input(\"Enter materials-related question: \")\n",
    "user_input = \"What is the bandgap of stable BaTiO3?\"\n",
    "user_input = \"What is the elastic tensor of NaCl?\"\n",
    "# user_input = \"What is the bandgap of stable MgSe?\"\n",
    "# user_input = \"Is YbCl3 magnetic or non-magnetic?\"\n",
    "# user_input = \"What is the energy above hull of Cu3P4Pb3O16 at 0K?\"\n",
    "# user_input = \"Is Cu stable in FCC or BCC structure at 0K based on the energy distance above hull?\"\n",
    "# user_input = \"Do we have stable Cu-substituted lead phosphate apatite at 0K?\"\n",
    "# user_input = \"What is the bandgap of the most stable magnetite?\"\n",
    "# user_input = \"Search for the necessary information for FCC Cu and summarize for me.\"\n",
    "\n",
    "mpllm.run_material_conversation(user_input=user_input, model='gpt-3.5-turbo-16k-0613', debug=False)\n",
    "# mpllm.run_material_conversation(user_input, model=\"gpt-3.5-turbo-16k-0613\", debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198fa7f",
   "metadata": {},
   "source": [
    "# Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openapi_schema_pydantic import PathItem, Operation, Parameter\n",
    "\n",
    "# class MPAPISpec:\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             force_download: bool = False,\n",
    "#             ):\n",
    "\n",
    "#         if force_download:\n",
    "#             spec = OpenAPISpec.from_url(\"https://api.materialsproject.org/openapi.json\")\n",
    "#             spec.\n",
    "\n",
    "# TODO: construct MPAPISpec class from OpenAPISpec\n",
    "\n",
    "\n",
    "def get_endpoints(spec: OpenAPISpec):\n",
    "    endpoints = [\n",
    "        (route, method, operation)\n",
    "        for route, paths in spec.paths.items()\n",
    "        for method, operation in paths\n",
    "        if method in [\"get\", \"post\"]\n",
    "    ]\n",
    "    return endpoints\n",
    "\n",
    "\n",
    "def get_functions(spec: OpenAPISpec):\n",
    "    endpoints = get_endpoints(spec)\n",
    "    functions = []\n",
    "    for route, method, operation in endpoints:\n",
    "        if operation is None:\n",
    "            continue\n",
    "\n",
    "        name = (\n",
    "            operation.operationId\n",
    "            if len(operation.operationId) <= 64\n",
    "            else operation.operationId[:64]\n",
    "        )\n",
    "\n",
    "        description = operation.description or operation.summary\n",
    "\n",
    "        properties = {\n",
    "            property.name: {\n",
    "                \"type\": property.param_schema.type,\n",
    "                \"description\": property.description\n",
    "                if property.description is not None\n",
    "                else \"\",\n",
    "                **(\n",
    "                    {\"enum\": property.param_schema.enum}\n",
    "                    if property.param_schema.enum is not None\n",
    "                    else {}\n",
    "                ),\n",
    "            }\n",
    "            for property in operation.parameters\n",
    "            if property.param_schema.type is not None\n",
    "        }\n",
    "\n",
    "        required = [\n",
    "            property.name for property in operation.parameters if property.required\n",
    "        ]\n",
    "        functions.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": properties,\n",
    "                },\n",
    "                **({\"required\": required} if required else {}),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return functions\n",
    "\n",
    "\n",
    "functions = get_functions(spec)\n",
    "\n",
    "print(len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"passed_functions.json\", \"w\") as f:\n",
    "#     json.dump(functions, f, indent=4)\n",
    "\n",
    "# functions\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"select_functions.json\", \"r\") as f:\n",
    "    functions = json.load(f)\n",
    "\n",
    "len(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e84068-df15-4050-9e9d-ccea43f82079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# from mp_api.client import MPRester\n",
    "\n",
    "# from langchain.tools import APIOperation, OpenAPISpec\n",
    "\n",
    "# load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "# MP_API_KEY = os.getenv(\"MP_API_KEY\", None)\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# spec = OpenAPISpec.from_url(\"https://api.materialsproject.org/openapi.json\")\n",
    "# operation = APIOperation.from_openapi_spec(\n",
    "#     spec, \"/materials/summary/{material_id}/\", \"get\"\n",
    "# )\n",
    "\n",
    "# # from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "# # from langchain.schema import HumanMessage, AIMessage, ChatMessage\n",
    "# # from langchain.tools import format_tool_to_openai_function\n",
    "\n",
    "# # llm = ChatAnthropic(anthropic_api_key=)\n",
    "# # llm = ChatOpenAI(model='gpt-4-0613', temperature=0.5, openai_api_key=OPENAI_API_KEY, client=)\n",
    "\n",
    "\n",
    "# class LLMaterialsAgent:\n",
    "#     def __init__(self, mp_api_key=MP_API_KEY, openai_api_key=OPENAI_API_KEY):\n",
    "#         # Initialize the Materials Project API\n",
    "#         self.mpr = MPRester(mp_api_key)\n",
    "#         # Initialize the OpenAI API\n",
    "#         openai.api_key = openai_api_key\n",
    "\n",
    "#     def get_materials_data(self, query_params):\n",
    "#         # Retrieve data from the Materials Project using the MPRester class\n",
    "#         data = self.mpr.summary.search(**query_params)\n",
    "#         return data\n",
    "\n",
    "#     def run_conversation(self, user_input):\n",
    "#         # Step 1: send the conversation and available functions to GPT\n",
    "#         messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "#         functions = [\n",
    "#             {\n",
    "#                 \"name\": \"get_materials_data\",\n",
    "#                 \"description\": \"Get materials data from the Materials Project\",\n",
    "#                 \"parameters\": {\n",
    "#                     \"type\": \"object\",\n",
    "#                     \"properties\": {\n",
    "#                         \"elements\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\"type\": \"string\"},\n",
    "#                             \"description\": \"Elements to query, e.g., ['Si', 'O']\",\n",
    "#                         },\n",
    "#                         \"band_gap\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\"type\": \"number\"},\n",
    "#                             \"description\": \"Range of band gap values, e.g., [0.5, 1.0]\",\n",
    "#                         },\n",
    "#                         \"fields\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\"type\": \"string\"},\n",
    "#                             \"description\": f\"Fields to return, including {self.mpr.summary.available_fields}\",\n",
    "#                         }\n",
    "#                         # \"limit\": {\n",
    "#                         #     \"type\": \"integer\",\n",
    "#                         #     \"description\": \"Number of materials to retrieve, e.g., 5\",\n",
    "#                         # },\n",
    "#                     },\n",
    "#                     \"required\": [\"elements\", \"band_gap\"],\n",
    "#                 },\n",
    "#             }\n",
    "#         ]\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-3.5-turbo-0613\",\n",
    "#             messages=messages,\n",
    "#             functions=functions,\n",
    "#             function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "#         )\n",
    "#         response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "#         # Step 2: check if GPT wanted to call a function\n",
    "#         if response_message.get(\"function_call\"):\n",
    "#             # Step 3: call the function\n",
    "#             # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "#             available_functions = {\n",
    "#                 \"get_materials_data\": self.get_materials_data,\n",
    "#             }  # only one function in this example, but you can have multiple\n",
    "#             function_name = response_message[\"function_call\"][\"name\"]\n",
    "#             function_to_call = available_functions[function_name]\n",
    "#             function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "#             function_response = function_to_call(query_params=function_args)\n",
    "\n",
    "#             # breakpoint()\n",
    "\n",
    "#             # Step 4: send the info on the function call and function response to GPT\n",
    "#             messages.append(\n",
    "#                 response_message\n",
    "#             )  # extend conversation with assistant's reply\n",
    "#             # breakpoint()\n",
    "#             messages.append(\n",
    "#                 {\n",
    "#                     \"role\": \"function\",\n",
    "#                     \"name\": function_name,\n",
    "#                     \"content\": json.dumps(function_response[0]),\n",
    "#                     # \"content\": function_response,\n",
    "#                 }\n",
    "#             )  # extend conversation with function response\n",
    "#             # breakpoint()\n",
    "#             second_response = openai.ChatCompletion.create(\n",
    "#                 model=\"gpt-3.5-turbo-0613\",\n",
    "#                 messages=messages,\n",
    "#             )  # get a new response from GPT where it can see the function response\n",
    "#             # breakpoint()\n",
    "#             return second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427454eb-7708-41ff-8b47-0607d836bd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# user_input = input(\"Please enter your natural language query: \")\n",
    "\n",
    "# user_input = \"Is UCl3 a stable material according to the thermodynamic hull on Materials Project?\"\n",
    "\n",
    "user_input = \"What is the bandgap of diamond based on Materials Project's data?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a data-vigilent agent that responds user requrests based on data hosted on Materials Project.\"\n",
    "            \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. Only use the functions you have been provided with.\"\n",
    "            \"Remember to provide `fields` or `_fields` to retrive as small request as possible.\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "]\n",
    "\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    # model=\"gpt-3.5-turbo-0613\",\n",
    "    model=\"gpt-3.5-turbo-16k-0613\",\n",
    "    # model=\"gpt-4-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    ")\n",
    "response_message = response[\"choices\"][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae61be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# passed_functions = []\n",
    "# for n in tqdm(range(len(functions))):\n",
    "#     try:\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-3.5-turbo-0613\",\n",
    "#             messages=messages,\n",
    "#             functions=[functions[n]],\n",
    "#             function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "#         )\n",
    "#         response_message = response[\"choices\"][0][\"message\"]\n",
    "#     except Exception as e:\n",
    "#         print(functions[n][\"name\"])\n",
    "#         continue\n",
    "#     passed_functions.append(functions[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf11272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(lambda x: x['name'], functions)).index('search_materials_synthesis__get')\n",
    "\n",
    "# functions.pop(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86962426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"passed_functions.json\", \"w\") as f:\n",
    "#     json.dump(functions, f, indent=4)\n",
    "\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x[\"name\"], functions)).index(\"search_materials_summary__get\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b2407",
   "metadata": {},
   "source": [
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_api.client.core import BaseRester\n",
    "from mp_api.client.routes import *\n",
    "\n",
    "\n",
    "class MPLLM:\n",
    "    def __init__(self, mp_api_key=MP_API_KEY, openai_api_key=OPENAI_API_KEY) -> None:\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "        self.base = BaseRester(\n",
    "            api_key=mp_api_key, monty_decode=True, use_document_model=False\n",
    "        )\n",
    "\n",
    "        self.materials = MaterialsRester(\n",
    "            api_key=mp_api_key, monty_decode=False, use_document_model=False\n",
    "        )\n",
    "\n",
    "        self.thermo = ThermoRester(\n",
    "            api_key=mp_api_key, monty_decode=False, use_document_model=False\n",
    "        )\n",
    "\n",
    "        self.tasks = TaskRester(\n",
    "            api_key=mp_api_key,\n",
    "        )\n",
    "\n",
    "        self.summary = SummaryRester(\n",
    "            api_key=mp_api_key, monty_decode=False, use_document_model=False\n",
    "        )\n",
    "\n",
    "    def search_base(self, query_params):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.base._search(all_fields=False, **query_params)\n",
    "\n",
    "    def search_materials_core(self, query_params: dict):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.materials._search(all_fields=False, **query_params)\n",
    "\n",
    "    def search_materials_summary(self, query_params: dict):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.summary._search(all_fields=False, **query_params)\n",
    "\n",
    "    def search_materials_thermo(self, query_params):\n",
    "        query_params[\"fields\"] = query_params.get(\"fields\", \"\").split(\",\")\n",
    "        _fields = query_params.pop(\"_fields\", None)\n",
    "        if _fields:\n",
    "            query_params[\"fields\"] += _fields.split(\",\")\n",
    "        return self.thermo._search(all_fields=False, **query_params)\n",
    "\n",
    "\n",
    "mpllm = MPLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4c6ba-15f2-434c-8c3a-59c6b1cb5660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "    \"search_base\": mpllm.search_base,\n",
    "    \"search_materials_thermo__get\": mpllm.search_materials_thermo,\n",
    "    \"search_materials_core__get\": mpllm.search_materials_core,\n",
    "    \"search_materials_summary_stats__get\": mpllm.search_materials_core,\n",
    "    \"search_materials_summary__get\": mpllm.search_materials_summary,\n",
    "}  # only one function in this example, but you can have multiple\n",
    "function_name = response_message[\"function_call\"][\"name\"]\n",
    "function_to_call = available_functions[function_name]\n",
    "function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "function_response = function_to_call(query_params=function_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(function_to_call)\n",
    "print(function_args)\n",
    "\n",
    "response_message[\"function_call\"][\"arguments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monty.json import MontyEncoder, MontyDecoder\n",
    "\n",
    "# json.dumps(function_response[0], cls=MontyEncoder)\n",
    "# json.dumps(function_response[0].dict(), indent=4, cls=MontyDecoder)\n",
    "# function_response[0]\n",
    "# MontyEncoder().encode(function_response[0])\n",
    "# MontyDecoder().decode(function_response[0].__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e88f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7264c-6a2e-4461-b069-831d85480c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = json.dumps(function_response)\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"function\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": content,\n",
    "        # \"content\": function_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16860c9b-d7bb-4009-a048-973744d4765d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    ")  # get a new response from GPT where it can see the function response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffaad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214ab19-0304-489d-b5ed-b948cb87d444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymatgen.core import Structure, Element\n",
    "import datetime\n",
    "\n",
    "\n",
    "def serialize_nested_object(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        serialized_dict = {}\n",
    "        for key, value in obj.items():\n",
    "            serialized_dict[key] = serialize_nested_object(value)\n",
    "        return serialized_dict\n",
    "    elif isinstance(obj, list):\n",
    "        serialized_list = []\n",
    "        for item in obj:\n",
    "            serialized_list.append(serialize_nested_object(item))\n",
    "        return serialized_list\n",
    "    elif isinstance(obj, (datetime.datetime, Element)):\n",
    "        return str(obj)\n",
    "    elif isinstance(obj, (str, int, float)):\n",
    "        return obj\n",
    "    else:\n",
    "        # Handle other data types or custom objects as needed\n",
    "        return str(obj)\n",
    "\n",
    "\n",
    "serialize_nested_object(function_response[0].dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467a881-d1f7-443e-be86-bd123fd65740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monty.json import MontyEncoder\n",
    "\n",
    "encoder = MontyEncoder()\n",
    "a = encoder.encode(function_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf2dbc-d1e4-4f38-a690-78052d904c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monty.json import MontyDecoder, MontyEncoder\n",
    "\n",
    "decoder = MontyDecoder()\n",
    "decoder.decode(function_response[0].dict())\n",
    "# encoder = MontyEncoder()\n",
    "# encoder.encode()\n",
    "# json.dumps(function_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053d321-0599-4f20-9ffa-e0464ec4f3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
