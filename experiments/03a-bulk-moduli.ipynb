{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from monty.tempfile import ScratchDir\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.agents import load_tools\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain.agents import create_react_agent, AgentExecutor, create_openai_functions_agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from llamp.mp.agents import (\n",
    "    MPSummaryExpert,\n",
    "    MPThermoExpert,\n",
    "    MPElasticityExpert,\n",
    "    MPDielectricExpert,\n",
    "    MPPiezoelectricExpert,\n",
    "    MPMagnetismExpert,\n",
    "    MPElectronicExpert,\n",
    "    MPSynthesisExpert,\n",
    "    MPStructureRetriever,\n",
    ")\n",
    "from llamp.atomate2.agents import MLFFMDAgent\n",
    "# from llamp.arxiv.agents import ArxivAgent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MP_API_KEY = os.getenv(\"MP_API_KEY\", None)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "OPENAI_ORGANIZATION = os.getenv(\"OPENAI_ORGANIZATION\", None)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", None)\n",
    "\n",
    "OPENAI_GPT_MODEL = \"gpt-4-1106-preview\"\n",
    "# OPENAI_GPT_MODEL = \"gpt-4-0125-preview\"\n",
    "# OPENAI_GPT_MODEL = \"gpt-3.5-turbo-1106\"\n",
    "# OPENAI_GPT_MODEL = \"gpt-4-turbo\"\n",
    "llama = True # TODO: if we're running llama instead of gpt backbone for llamp\n",
    "react = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not llama:\n",
    "    top_llm = ChatOpenAI(\n",
    "        temperature=0.1,\n",
    "        model=OPENAI_GPT_MODEL,\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        openai_organization=OPENAI_ORGANIZATION,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    )\n",
    "else: \n",
    "    model = \"llama3.1\"\n",
    "    top_llm = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=0.1,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        num_predict=-2,\n",
    "    )\n",
    "\n",
    "\n",
    "bottom_callback_handler = StreamingStdOutCallbackHandler()\n",
    "\n",
    "if not llama:\n",
    "    bottom_llm = ChatOpenAI(\n",
    "        temperature=0.0,\n",
    "        model=OPENAI_GPT_MODEL,\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        openai_organization=OPENAI_ORGANIZATION,\n",
    "        max_retries=5,\n",
    "        streaming=True,\n",
    "        callbacks=[bottom_callback_handler],\n",
    "    )\n",
    "else:\n",
    "       bottom_llm =  ChatOllama(\n",
    "        model=model,\n",
    "        temperature=0.1,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        num_predict=-2,\n",
    "    )\n",
    "\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper())\n",
    "\n",
    "tools = load_tools([\"llm-math\"], llm=bottom_llm)\n",
    "tools += [PythonREPLTool(\n",
    "    description=re.sub(\n",
    "        r\"\\s+\",\n",
    "        \" \",\n",
    "        \"\"\"A Python shell. Use this to execute python commands.\n",
    "        Input should be a valid python command.\n",
    "        \n",
    "        TIPS:\n",
    "        - If you want to see the output of a value, you should print it out\n",
    "        with `print(...)`.\n",
    "        - If you write any file to a local file system, you should ALWAYS \n",
    "        print the paths to the files as well.\n",
    "        - If you plot any graph, you should show the graph, save it to local \n",
    "        and print the path to the image file.\n",
    "        - Combine all your output string together and print once together in\n",
    "        JSON format at the end of the code.\n",
    "        \"\"\"\n",
    "    )\n",
    ")]\n",
    "tools += [\n",
    "    MPThermoExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPElasticityExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPDielectricExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPMagnetismExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPElectronicExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPPiezoelectricExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPSummaryExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPSynthesisExpert(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MPStructureRetriever(llm=bottom_llm, mp_api_key=MP_API_KEY).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    MLFFMDAgent(llm=bottom_llm).as_tool(\n",
    "        agent_kwargs=dict(return_intermediate_steps=False)\n",
    "    ),\n",
    "    arxiv,\n",
    "    wikipedia,\n",
    "]\n",
    "\n",
    "instructions = re.sub(\n",
    "    r\"\\s+\",\n",
    "    \" \",\n",
    "    \"\"\"You name is LLaMP and you are a helpful agent that can consult \n",
    "    materials-related data through Materials Project (MP) database, arXiv, \n",
    "    Wikipedia, and run calculations/simulations via a python REPL and MP atomate2 \n",
    "    computational workflow.     \n",
    "        \n",
    "    If you get an error, debug your code and try again. Only use \n",
    "    the output of your code to answer the question. Ask user to clarify their \n",
    "    queries if needed. Please note that you don't have direct control over MP but \n",
    "    through multiple assistant agents to help you. You need to provide complete \n",
    "    context in the input for assistants to do their job. REFINE and CRITQUE the \n",
    "    output of the assistants if needed. You can end the conversation by saying \"Bye\".\n",
    "    \"\"\",\n",
    "    )\n",
    "if react:\n",
    "    base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
    "    prompt = base_prompt.partial(instructions=instructions)\n",
    "else:\n",
    "    base_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "    prompt = base_prompt.partial(instructions=instructions) \n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True\n",
    ")\n",
    "\n",
    "if react:\n",
    "    agent = create_react_agent(top_llm, tools, prompt)\n",
    "else:\n",
    "    agent = create_openai_functions_agent(top_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=conversational_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \n",
      "What are the bulk moduli of the following metals: Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "The bulk modulus (K) is a measure of the resistance of a material to compression. Here are the bulk moduli for the listed metals:\n",
      "\n",
      "1. **Scandium (Sc)**: 44 GPa\n",
      "2. **Titanium (Ti)**: 110-120 GPa (note: this value can vary slightly depending on the specific alloy or crystal structure)\n",
      "3. **Vanadium (V)**: 204 GPa\n",
      "4. **Chromium (Cr)**: 263 GPa\n",
      "5. **Manganese (Mn)**: 193 GPa\n",
      "6. **Iron (Fe)**: 170-180 GPa (note: this value can vary slightly depending on the specific alloy or crystal structure)\n",
      "7. **Cobalt (Co)**: 207 GPa\n",
      "8. **Nickel (Ni)**: 200 GPa\n",
      "9. **Copper (Cu)**: 137 GPa\n",
      "10. **Zinc (Zn)**: 65-70 GPa\n",
      "\n",
      "Please note that these values are approximate and can vary slightly depending on the specific alloy, crystal structure, or other factors.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* CRC Handbook of Chemistry and Physics (various editions)\n",
      "* ASM Handbook, Volume 2: Properties and Selection: Nonferrous Alloys and Special-Purpose Materials (2008)\n",
      "* Various online databases and scientific articles.\u001b[32;1m\u001b[1;3mThe bulk modulus (K) is a measure of the resistance of a material to compression. Here are the bulk moduli for the listed metals:\n",
      "\n",
      "1. **Scandium (Sc)**: 44 GPa\n",
      "2. **Titanium (Ti)**: 110-120 GPa (note: this value can vary slightly depending on the specific alloy or crystal structure)\n",
      "3. **Vanadium (V)**: 204 GPa\n",
      "4. **Chromium (Cr)**: 263 GPa\n",
      "5. **Manganese (Mn)**: 193 GPa\n",
      "6. **Iron (Fe)**: 170-180 GPa (note: this value can vary slightly depending on the specific alloy or crystal structure)\n",
      "7. **Cobalt (Co)**: 207 GPa\n",
      "8. **Nickel (Ni)**: 200 GPa\n",
      "9. **Copper (Cu)**: 137 GPa\n",
      "10. **Zinc (Zn)**: 65-70 GPa\n",
      "\n",
      "Please note that these values are approximate and can vary slightly depending on the specific alloy, crystal structure, or other factors.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* CRC Handbook of Chemistry and Physics (various editions)\n",
      "* ASM Handbook, Volume 2: Properties and Selection: Nonferrous Alloys and Special-Purpose Materials (2008)\n",
      "* Various online databases and scientific articles.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pattern = re.compile(r'\\b' + re.escape(\"Bye\") + r'\\b[.!?]*')\n",
    "\n",
    "query = \"\"\"\n",
    "What are the bulk moduli of the following metals: Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn?\n",
    "\"\"\"\n",
    "\n",
    "# with ScratchDir(rootpath=os.getcwd()):\n",
    "# with tempfile.TemporaryDirectory(dir=os.getcwd()) as tmpdir:\n",
    "#     os.chdir(tmpdir)\n",
    "#     while True:\n",
    "query = query or input(\"Query:\")\n",
    "print(f\"Query: {query}\")\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": query,\n",
    "    }\n",
    ")\n",
    "# if re.search(pattern, response[\"output\"]):\n",
    "#     break\n",
    "# query = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "print(agent_executor.memory.chat_memory.messages[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-1106\n",
      "The bulk moduli of the metals are as follows (in GPa):\n",
      "\n",
      "Sc: 120\n",
      "Ti: 110\n",
      "V: 160\n",
      "Cr: 160\n",
      "Mn: 120\n",
      "Fe: 170\n",
      "Co: 180\n",
      "Ni: 180\n",
      "Cu: 140\n",
      "Zn: 110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model=model,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_organization=OPENAI_ORGANIZATION,\n",
    ")\n",
    "\n",
    "response = llm.invoke(agent_executor.memory.chat_memory.messages[0].content)\n",
    "\n",
    "print(model)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n",
      "- Scandium (Sc) - 56.6 GPa\n",
      "- Titanium (Ti) - 110 GPa\n",
      "- Vanadium (V) - 160 GPa\n",
      "- Chromium (Cr) - 160 GPa\n",
      "- Manganese (Mn) - 120 GPa\n",
      "- Iron (Fe) - 170 GPa\n",
      "- Cobalt (Co) - 180 GPa\n",
      "- Nickel (Ni) - 180 GPa\n",
      "- Copper (Cu) - 140 GPa\n",
      "- Zinc (Zn) - 70 GPa\n",
      "\n",
      "Please note that these are approximate values and can vary slightly depending on the specific conditions (such as temperature and pressure).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"gpt-4\"\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model=model,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_organization=OPENAI_ORGANIZATION,\n",
    ")\n",
    "\n",
    "response = llm.invoke(agent_executor.memory.chat_memory.messages[0].content)\n",
    "\n",
    "print(model)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-pro\n",
      "| Metal | Bulk Modulus (GPa) |\n",
      "|---|---|\n",
      "| Sc | 68 |\n",
      "| Ti | 110 |\n",
      "| V | 162 |\n",
      "| Cr | 279 |\n",
      "| Mn | 122 |\n",
      "| Fe | 160 |\n",
      "| Co | 199 |\n",
      "| Ni | 192 |\n",
      "| Cu | 130 |\n",
      "| Zn | 65 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"gemini-pro\"\n",
    "llm = GoogleGenerativeAI(\n",
    "    temperature=0.7,\n",
    "    model=f\"models/{model}\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "response = llm.invoke(agent_executor.memory.chat_memory.messages[0].content)\n",
    "\n",
    "print(model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3\n",
      "Here are the bulk moduli (Young's modulus) for the listed metals:\n",
      "\n",
      "1. Scandium (Sc): 68-72 GPa [1]\n",
      "2. Titanium (Ti): 165 GPa [2]\n",
      "3. Vanadium (V): 235-240 GPa [3]\n",
      "4. Chromium (Cr): 270-280 GPa [4]\n",
      "5. Manganese (Mn): 184-200 GPa [5]\n",
      "6. Iron (Fe): 211-224 GPa [6]\n",
      "7. Cobalt (Co): 315-330 GPa [7]\n",
      "8. Nickel (Ni): 204-210 GPa [8]\n",
      "9. Copper (Cu): 115-120 GPa [9]\n",
      "10. Zinc (Zn): 130-140 GPa [10]\n",
      "\n",
      "Please note that the values may vary slightly depending on the source and specific crystal structure of the metal.\n",
      "\n",
      "References:\n",
      "\n",
      "[1] Scandium: Landa et al., Phys. Rev. B, 64 (2001), 245106.\n",
      "[2] Titanium: Wang et al., Acta Mater., 55 (2007), 3843-3854.\n",
      "[3] Vanadium: Vitos et al., Phys. Rev. B, 65 (2002), 144103.\n",
      "[4] Chromium: Liu et al., J. Appl. Phys., 95 (2004), 1716-1721.\n",
      "[5] Manganese: Chen et al., Acta Mater., 56 (2008), 3453-3461.\n",
      "[6] Iron: Wang et al., Phys. Rev. B, 73 (2006), 134201.\n",
      "[7] Cobalt: Li et al., J. Appl. Phys., 102 (2007), 113909.\n",
      "[8] Nickel: Liu et al., Acta Mater., 54 (2006), 3473-3482.\n",
      "[9] Copper: Gao et al., Phys. Rev. B, 72 (2005), 174103.\n",
      "[10] Zinc: Kim et al., J. Appl. Phys., 93 (2003), 4444-4451.\n",
      "\n",
      "These values are approximate and might have some uncertainties due to experimental methods used.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3\"\n",
    "llm = ChatOllama(\n",
    "    model=model,\n",
    "    temperature=0.7,\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    num_predict=-2,\n",
    ")\n",
    "\n",
    "response = llm.invoke(agent_executor.memory.chat_memory.messages[0].content)\n",
    "\n",
    "print(model)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09720528814f436bb06e81236e4f1fb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0c489d7f1bba4246a03fc5bb3949431a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ed9f39d48b24cbcb1c4ba787fb00fdd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5cd2770d179848c8b87f0f6f96783141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6a9556c1be39471aa43008c35508b71b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7de9954e1f944a249843ac13b8028031",
       "style": "IPY_MODEL_5cd2770d179848c8b87f0f6f96783141",
       "value": " 31/31 [00:00&lt;00:00, 4128.25it/s]"
      }
     },
     "6c07de7087514f8d9a0103f332fd9117": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a22922a2105c43939cf381ef14fc3441",
        "IPY_MODEL_dbfc8d2504e44cfa95af329cb733551d",
        "IPY_MODEL_6a9556c1be39471aa43008c35508b71b"
       ],
       "layout": "IPY_MODEL_2ed9f39d48b24cbcb1c4ba787fb00fdd"
      }
     },
     "7de9954e1f944a249843ac13b8028031": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a04e3a75c4d64fee9a315174007fb53a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a22922a2105c43939cf381ef14fc3441": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d3fe78eb10a74264b53ec006904e1032",
       "style": "IPY_MODEL_a04e3a75c4d64fee9a315174007fb53a",
       "value": "Retrieving ElasticityDoc documents: 100%"
      }
     },
     "d3fe78eb10a74264b53ec006904e1032": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dbfc8d2504e44cfa95af329cb733551d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0c489d7f1bba4246a03fc5bb3949431a",
       "max": 31,
       "style": "IPY_MODEL_09720528814f436bb06e81236e4f1fb6",
       "value": 31
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
